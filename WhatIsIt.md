# What is Prompt Injection?

Prompt injection is a type of attack where a person manipulates a large language model (LLM) by injecting hidden or misleading instructions into the text it processes.  
This causes the AI to ignore its original rules and behave in unintended or harmful ways.

This type of attack is especially dangerous because LLMs interpret text directly as instructions. This means the model cannot easily tell whether a command is a genuine user request or a hidden trick added by an attacker. As a result, attackers can force the AI to reveal confidential information, bypass safety filters, or generate misleading or harmful outputs. For these reasons, prompt injection is considered one of the most significant security challenges in modern AI systems.
